{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"First we import the necessary libraries\"\"\"\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Grab our csv files and import as DataFrames\"\"\"\n",
    "SP500 = pd.read_csv('../Data/SP500_new.csv', parse_dates=True)\n",
    "Nasdaq = pd.read_csv('../Data/Nasdaq_new.csv', parse_dates=True)\n",
    "DJI = pd.read_csv('../Data/DJI_new.csv', parse_dates=True)\n",
    "DAX = pd.read_csv('../Data/DAX_new.csv', parse_dates=True)\n",
    "Paris = pd.read_csv('../Data/Paris_new.csv', parse_dates=True)\n",
    "Tokyo = pd.read_csv('../Data/Tokyo_new.csv', parse_dates=True)\n",
    "HongKong = pd.read_csv('../Data/HongKong_new.csv', parse_dates=True)\n",
    "Aus = pd.read_csv('../Data/Aus_new.csv', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Our target variable is tomorrow's Adj Close\"\"\"\n",
    "target_raw = (SP500['Adj Close'].shift(-1)/SP500['Adj Close'])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The generate_features function performs feature engineering using Adj Close, the features generated are Daily Returns, \n",
    "Momentum (Daily Returns over 2 days), Daily Return SMA and lagging Daily Returns\n",
    "\"\"\"\n",
    "datasets = [SP500, Nasdaq, DJI, DAX, Paris, Tokyo, HongKong, Aus]\n",
    "names = ['SP500', 'Nasdaq', 'DJI', 'DAX', 'Paris', 'Tokyo', 'HongKong', 'Aus']\n",
    "\n",
    "def generate_features(datasets, DR, DR_SMA, Lagging):\n",
    "    Max = max(DR, DR_SMA, Lagging+1)\n",
    "    for i in range(len(datasets)):\n",
    "        dataset = datasets[i]\n",
    "        name = names[i]\n",
    "        for j in range(1, DR+1):\n",
    "            dataset[name+'_'+str(j)+'DailyReturn'] = (dataset['Adj Close']/dataset['Adj Close'].shift(j))-1\n",
    "        for k in range(2, DR_SMA+1):\n",
    "            dataset[name+'_'+str(k)+'DR_SMA'] = pd.rolling_mean(dataset[name+'_'+str(1)+'DailyReturn'], window=k)\n",
    "        for l in range(1, Lagging+1):\n",
    "            dataset[name+'_'+str(l)+'LaggingDays'] = dataset[name+'_'+str(1)+'DailyReturn'].shift(l)\n",
    "        dataset.drop(dataset.index[:Max], inplace=True)\n",
    "    return Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"After feature engineering, merge all datasets and drop the 'useless' features\"\"\"\n",
    "def merge_datasets(datasets):\n",
    "    drop_features = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Date']\n",
    "    for i in range(len(datasets)):\n",
    "        datasets[i] = datasets[i].drop(drop_features, axis=1)\n",
    "    megaset = pd.concat(datasets, axis=1)\n",
    "    return megaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=2,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=3,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=4,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=5,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=6,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=7,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=8,center=False).mean()\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Leslie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(window=9,center=False).mean()\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "generate_features(datasets, 9, 9, 9)\n",
    "megaset = merge_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Label encode our target variable, 1 for increase, 0 for decrease or no change\"\"\"\n",
    "target = target_raw[Max:]\n",
    "target[target > 0] = 1\n",
    "target[target <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Split our megaset into training and cross-validation (test) subsets\"\"\"\n",
    "X_train = megaset[:-500]\n",
    "X_test = megaset[-500:-1]\n",
    "y_train = target[:-500]\n",
    "y_test = target[-500:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our training features is: (5743, 208)\n",
      "The size of our testing features is: (499, 208)\n",
      "The size of our training target is: (5743,)\n",
      "The size of our testing target is: (499,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Let's have a look at the dimensions of our training and testing sets\"\"\"\n",
    "print(\"The size of our training features is: {}\".format(X_train.shape))\n",
    "print(\"The size of our testing features is: {}\".format(X_test.shape))\n",
    "print(\"The size of our training target is: {}\".format(y_train.shape))\n",
    "print(\"The size of our testing target is: {}\".format(y_test.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
